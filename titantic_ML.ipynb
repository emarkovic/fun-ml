{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import (RandomForestClassifier, \n",
    "AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, \n",
    "VotingClassifier)\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.cross_validation import KFold\n",
    "from collections import Counter\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "title_list=['Mrs', \n",
    "            'Mr', \n",
    "            'Master', \n",
    "            'Miss', \n",
    "            'Major', \n",
    "            'Rev',\n",
    "            'Dr', \n",
    "            'Ms', \n",
    "            'Mlle',\n",
    "            'Col', \n",
    "            'Capt', \n",
    "            'Mme', \n",
    "            'Countess',\n",
    "            'Don', \n",
    "            'Jonkheer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sanitization helper functions\n",
    "def extract_title(name):\n",
    "    for title in title_list:\n",
    "        if (title in name):\n",
    "            return title\n",
    "\n",
    "def detect_outliers(df,n,features):\n",
    "    \"\"\"\n",
    "    Takes a dataframe df of features and returns a list of the indices\n",
    "    corresponding to the observations containing more than n outliers according\n",
    "    to the Tukey method.\n",
    "    \"\"\"\n",
    "    outlier_indices = []\n",
    "    # iterate over features(columns)\n",
    "    for col in features:\n",
    "        # 1st quartile (25%)\n",
    "        Q1 = np.percentile(df[col], 25)\n",
    "        # 3rd quartile (75%)\n",
    "        Q3 = np.percentile(df[col],75)\n",
    "        # Interquartile range (IQR)\n",
    "        IQR = Q3 - Q1\n",
    "        # outlier step\n",
    "        outlier_step = 1.5 * IQR\n",
    "        # Determine a list of indices of outliers for feature col\n",
    "        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step )].index\n",
    "        # append the found outlier indices for col to the list of outlier indices \n",
    "        outlier_indices.extend(outlier_list_col)\n",
    "    # select observations containing more than 2 outliers\n",
    "    outlier_indices = Counter(outlier_indices)        \n",
    "    multiple_outliers = list( k for k, v in outlier_indices.items() if v > n )\n",
    "    return multiple_outliers\n",
    "        \n",
    "# Sanitization functions\n",
    "def add_family_size(input_df):\n",
    "    df = input_df.copy()\n",
    "    return (df.assign(family_size=lambda df: df.SibSp + df.Parch + 1))\n",
    "\n",
    "def add_age_class(input_df):\n",
    "    df = input_df.copy()\n",
    "    return (df.assign(age_class=lambda df: df.Age * df.Pclass))\n",
    "\n",
    "def add_gender_class(input_df):\n",
    "    df = input_df.copy()\n",
    "    return (df.assign(gender_class=lambda df: df.Sex + df.Pclass.apply(str)))\n",
    "\n",
    "def sanitize_name_to_title(input_df):\n",
    "    df = input_df.copy()\n",
    "    df = df.assign(Title=lambda df: df.Name.map(lambda name: extract_title(name)))\n",
    "    return df.drop('Name', axis=1)\n",
    "\n",
    "def one_hot(input_df):\n",
    "    df = input_df.copy()\n",
    "    # titles\n",
    "    titles_ohe = pd.get_dummies(df.Title, prefix=\"Title\")\n",
    "    df = pd.concat([df, titles_ohe], 1)\n",
    "    # sex\n",
    "    sex_ohe = pd.get_dummies(df.Sex, prefix=\"Sex\")\n",
    "    df = pd.concat([df, sex_ohe], 1)\n",
    "    # section\n",
    "    section_ohe = pd.get_dummies(df.Section, prefix=\"Section\")\n",
    "    df = pd.concat([df, section_ohe], 1)\n",
    "    # pclass\n",
    "    pclass_ohe = pd.get_dummies(df.Pclass, prefix=\"Pclass\")\n",
    "    df = pd.concat([df, pclass_ohe], 1)\n",
    "    # embarked\n",
    "    embarked_ohe = pd.get_dummies(df.Embarked, prefix=\"Embarked\")\n",
    "    df = pd.concat([df, embarked_ohe], 1)\n",
    "    # gender class\n",
    "    genderClass_ohe = pd.get_dummies(df.gender_class, prefix=\"GenClass\")\n",
    "    df = pd.concat([df, genderClass_ohe], 1)\n",
    "    return df.reset_index(drop=True)\n",
    "    \n",
    "# Prepare data for ml\n",
    "def prep_data_for_ML(input_df):\n",
    "    df = input_df.copy()\n",
    "    # initial sanitization \n",
    "    df = (df.pipe(add_family_size) # add a column for sib+parch\n",
    "          .pipe(add_age_class)   # add a column for age*pclass\n",
    "          .pipe(sanitize_name_to_title) # get rid of names and extract title\n",
    "          .pipe(add_gender_class)) # add a column for sex+str(pclass)\n",
    "    # separating cabin section from cabin number     \n",
    "    df[\"Section\"] = df.Cabin.str[:1]\n",
    "    df[\"CabinNumber\"] = df.Cabin.str.extract(\"(\\d+)\")\n",
    "    df = one_hot(df)\n",
    "    df = df.fillna(0)\n",
    "    return df.drop(['Title', 'Sex', 'Section', 'Pclass', 'Cabin', 'Embarked', 'Ticket', 'gender_class'], axis=1)\n",
    "\n",
    "# Check your accuracy\n",
    "def mean_accuracy(x_all, y_all):\n",
    "    kf = KFold(891, n_folds=10)\n",
    "    outcomes = []\n",
    "    fold = 0\n",
    "    for train_index, test_index in kf:\n",
    "        fold += 1\n",
    "        X_train, X_test = x_all.values[train_index], x_all.values[test_index]\n",
    "        y_train, y_test = y_all.values[train_index], y_all.values[test_index]\n",
    "        classifier.fit(X_train, y_train)\n",
    "        predictions = classifier.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, predictions)\n",
    "        outcomes.append(accuracy)\n",
    "        print(\"Fold {0} accuracy: {1}\".format(fold, accuracy))     \n",
    "    mean_outcome = np.mean(outcomes)\n",
    "    print(\"Mean Accuracy: {0}\".format(mean_outcome)) \n",
    "    \n",
    "# Grid Search\n",
    "def run_ml(classifier, parameters, x_train, y_train):\n",
    "    # Type of scoring used to compare parameter combinations\n",
    "    acc_scorer = make_scorer(accuracy_score)\n",
    "    # Run the grid search\n",
    "    # Grid search finds the best combo of parameters\n",
    "    # It randomly splits the train and validation set therefore results of each would be different\n",
    "    grid_obj = GridSearchCV(classifier, parameters, scoring=acc_scorer)\n",
    "    grid_obj = grid_obj.fit(x_train, y_train)\n",
    "    classifier = grid_obj.best_estimator_\n",
    "    # Fit the best algorithm to the data.\n",
    "    print(grid_obj.best_estimator_)\n",
    "    print(classifier.score(x_train, y_train))\n",
    "    return classifier\n",
    "\n",
    "def get_data_for_ML(train, test):\n",
    "    train_csv = train.copy()\n",
    "    test_csv = test.copy()\n",
    "    \n",
    "    train = prep_data_for_ML(train_csv)\n",
    "    y_train = train.Survived\n",
    "    X_train = train.drop(\"Survived\", axis=1)\n",
    "    \n",
    "    test = prep_data_for_ML(test_csv)\n",
    "    # test data is missing the following features:\n",
    "    test['Title_Capt'] = 0\n",
    "    test['Title_Countess'] = 0\n",
    "    test['Title_Jonkheer'] = 0\n",
    "    test['Title_Major'] = 0\n",
    "    test['Title_Mlle'] = 0\n",
    "    test['Title_Mme'] = 0\n",
    "    test['Section_T'] = 0\n",
    "    \n",
    "    return X_train, y_train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Functions for \"Titanic top 4\" kernel\n",
    "\n",
    "# Helper functions \n",
    "def detect_outliers(df,n,features):\n",
    "    \"\"\"\n",
    "    Takes a dataframe df of features and returns a list of the indices\n",
    "    corresponding to the observations containing more than n outliers according\n",
    "    to the Tukey method.\n",
    "    \"\"\"\n",
    "    outlier_indices = []\n",
    "    # iterate over features(columns)\n",
    "    for col in features:\n",
    "        # 1st quartile (25%)\n",
    "        Q1 = np.percentile(df[col], 25)\n",
    "        # 3rd quartile (75%)\n",
    "        Q3 = np.percentile(df[col],75)\n",
    "        # Interquartile range (IQR)\n",
    "        IQR = Q3 - Q1\n",
    "        # outlier step\n",
    "        outlier_step = 1.5 * IQR\n",
    "        # Determine a list of indices of outliers for feature col\n",
    "        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step )].index\n",
    "        # append the found outlier indices for col to the list of outlier indices \n",
    "        outlier_indices.extend(outlier_list_col)\n",
    "    # select observations containing more than 2 outliers\n",
    "    outlier_indices = Counter(outlier_indices)        \n",
    "    multiple_outliers = list( k for k, v in outlier_indices.items() if v > n )\n",
    "    return multiple_outliers\n",
    "\n",
    "# Sanitization\n",
    "def drop_outliers(input_df):\n",
    "    df = input_df.copy()\n",
    "    outliers = detect_outliers(train,2,[\"Age\",\"SibSp\",\"Parch\",\"Fare\"])\n",
    "    return df.drop(outliers, axis = 0).reset_index(drop=True)\n",
    "\n",
    "def fill_in_age(input_df):\n",
    "    df = input_df.copy()\n",
    "    index_NaN_age = list(df[\"Age\"][df[\"Age\"].isnull()].index)\n",
    "    \n",
    "    for i in index_NaN_age :\n",
    "        age_med = df[\"Age\"].median()\n",
    "        age_pred = df[\"Age\"][(\n",
    "            (df['SibSp'] == df.iloc[i][\"SibSp\"]) & \n",
    "            (df['Parch'] == df.iloc[i][\"Parch\"]) & \n",
    "            (df['Pclass'] == df.iloc[i][\"Pclass\"])\n",
    "        )].median()\n",
    "        \n",
    "        if not np.isnan(age_pred) :\n",
    "            df['Age'].iloc[i] = age_pred\n",
    "        else :\n",
    "            df['Age'].iloc[i] = age_med\n",
    "            \n",
    "    return df\n",
    "\n",
    "def sanitize_title(input_df):\n",
    "    df = sanitize_name_to_title(input_df)    \n",
    "    titles_to_replace = ['Lady', 'the Countess','Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona']\n",
    "    title_map = {\"Master\" : 0,\n",
    "                 \"Miss\" : 1,\n",
    "                 \"Ms\" : 1,\n",
    "                 \"Mme\" : 1,\n",
    "                 \"Mlle\" : 1,\n",
    "                 \"Mrs\" : 1,\n",
    "                 \"Mr\" : 2,\n",
    "                 \"Rare\" : 3}    \n",
    "    df[\"Title\"] = df[\"Title\"].replace(titles_to_replace, 'Rare')\n",
    "    df[\"Title\"] = df[\"Title\"].map(title_map)\n",
    "    df[\"Title\"] = df[\"Title\"].astype(int)\n",
    "    return df\n",
    "\n",
    "def add_family_buckets(input_df):\n",
    "    df = input_df.copy()\n",
    "    df['Single'] = df['family_size'].map(lambda s: 1 if s == 1 else 0)\n",
    "    df['SmallF'] = df['family_size'].map(lambda s: 1 if  s == 2  else 0)\n",
    "    df['MedF'] = df['family_size'].map(lambda s: 1 if 3 <= s <= 4 else 0)\n",
    "    df['LargeF'] = df['family_size'].map(lambda s: 1 if s >= 5 else 0)\n",
    "    return df\n",
    "\n",
    "def sanitize_ticket(input_df):\n",
    "    df = input_df.copy()    \n",
    "    Ticket = []\n",
    "    for i in list(df.Ticket):\n",
    "        if not i.isdigit() :\n",
    "            Ticket.append(i.replace(\".\",\"\").replace(\"/\",\"\").strip().split(' ')[0]) #Take prefix\n",
    "        else:\n",
    "            Ticket.append(\"X\")    \n",
    "    df[\"Ticket\"] = Ticket\n",
    "    return df\n",
    "\n",
    "def one_hot_2(input_df):\n",
    "    df = input_df.copy()    \n",
    "    df = pd.get_dummies(df, columns = [\"Title\"])\n",
    "    df = pd.get_dummies(df, columns = [\"Embarked\"], prefix=\"Em\")\n",
    "    df = pd.get_dummies(df, columns = [\"Cabin\"],prefix=\"Cabin\")\n",
    "    df = pd.get_dummies(df, columns = [\"Ticket\"], prefix=\"T\")\n",
    "    df = pd.get_dummies(df, columns = [\"Pclass\"],prefix=\"Pc\")\n",
    "    return df\n",
    "\n",
    "def prep_and_get_data_top_4(train, test):\n",
    "    train_csv = train.copy()\n",
    "    test_csv = test.copy()\n",
    "    \n",
    "    # Load and check data\n",
    "    train_csv = drop_outliers(train)\n",
    "    dataset = pd.concat(objs=[train, test], axis=0).reset_index(drop=True)\n",
    "    dataset = dataset.fillna(np.nan)\n",
    "    \n",
    "    # Feature analysis\n",
    "    dataset[\"Fare\"] = (dataset[\"Fare\"].fillna(dataset[\"Fare\"].median())\n",
    "                   .pipe(lambda fare: fare.map(lambda i: np.log(i) if i > 0 else 0)))\n",
    "    dataset[\"Embarked\"] = dataset[\"Embarked\"].fillna(\"S\")\n",
    "    \n",
    "    # Fill in missing values\n",
    "    dataset[\"Sex\"] = dataset[\"Sex\"].map({\"male\": 0, \"female\":1})\n",
    "    dataset = fill_in_age(dataset)\n",
    "    \n",
    "#     dataset = add_age_class(dataset)\n",
    "    \n",
    "    # Feature engineering\n",
    "    dataset = sanitize_title(dataset) # this drops \"name\" column\n",
    "    dataset = add_family_size(dataset)\n",
    "    dataset = add_family_buckets(dataset)\n",
    "    \n",
    "    # replacing blank cabin values with X\n",
    "    dataset[\"Cabin\"] = pd.Series([i[0] if not pd.isnull(i) else 'X' for i in dataset['Cabin'] ])\n",
    "    \n",
    "    # Ticket prefixed seem to indicate placement on the ship, get those prefixes\n",
    "    dataset = sanitize_ticket(dataset)\n",
    "    \n",
    "    # Categorical values for pclass\n",
    "    dataset[\"Pclass\"] = dataset[\"Pclass\"].astype(\"category\")\n",
    "    \n",
    "    dataset = one_hot_2(dataset)\n",
    "    \n",
    "    train_len = len(train)\n",
    "    train2 = dataset[:train_len]\n",
    "    test = dataset[train_len:]\n",
    "    test.drop(labels=[\"Survived\"],axis = 1,inplace=True)\n",
    "    train2[\"Survived\"] = train2[\"Survived\"].astype(int)\n",
    "    X_train = train2.drop(labels = [\"Survived\"],axis = 1)\n",
    "    y_train = train2[\"Survived\"]\n",
    "    \n",
    "    return X_train, y_train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"titanic_train.csv\")\n",
    "test = pd.read_csv(\"titanic_test.csv\")\n",
    "\n",
    "# original\n",
    "X_train, y_train, test = get_data_for_ML(train, test)\n",
    "\n",
    "# top 4 kernel\n",
    "# X_train, y_train, test = prep_and_get_data_top_4(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_c = X_train.copy()\n",
    "test_c = test.copy()\n",
    "\n",
    "test_pass_ids = test.PassengerId\n",
    "\n",
    "X_train_c = X_train_c.drop([\"PassengerId\"], axis=1)\n",
    "test_c = test_c.drop([\"PassengerId\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X_train_c\n",
    "test = test_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
      "            max_depth=10, max_features='sqrt', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=5,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=6, n_jobs=1,\n",
      "            oob_score=True, random_state=2, verbose=0, warm_start=False)\n",
      "0.910213243547\n"
     ]
    }
   ],
   "source": [
    "# Choose the type of classifier. \n",
    "rfc = RandomForestClassifier()\n",
    "\n",
    "# Choose some parameter combinations to try\n",
    "params_rfc = {'n_estimators': [4, 6, 9], \n",
    "              'max_features': ['log2', 'sqrt','auto'], \n",
    "              'criterion': ['entropy', 'gini'],\n",
    "              'max_depth': [2, 3, 5, 10], \n",
    "              'min_samples_split': [2, 3, 5],\n",
    "              'min_samples_leaf': [1,5,8],\n",
    "              'oob_score': [True],\n",
    "              'random_state' : [2]\n",
    "             }\n",
    "\n",
    "# Highest accuracy with this combo\n",
    "# RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
    "#             max_depth=10, max_features='auto', max_leaf_nodes=None,\n",
    "#             min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "#             min_samples_leaf=1, min_samples_split=2,\n",
    "#             min_weight_fraction_leaf=0.0, n_estimators=9, n_jobs=1,\n",
    "#             oob_score=True, random_state=None, verbose=0, warm_start=False)\n",
    "\n",
    "rfc = run_ml(rfc, params_rfc, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = rfc.predict(test)\n",
    "\n",
    "output = test_pass_ids.to_frame(\"PassengerId\")\n",
    "output = output.assign(Survived=pred)\n",
    "output.to_csv('output.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTreesClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
      "           max_depth=5, max_features='sqrt', max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=1, min_samples_split=5,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=4, n_jobs=1,\n",
      "           oob_score=True, random_state=2, verbose=0, warm_start=False)\n",
      "0.842873176207\n"
     ]
    }
   ],
   "source": [
    "# Extra Trees!\n",
    "etc = ExtraTreesClassifier()\n",
    "# Choose some parameter combinations to try\n",
    "params_etc = {'n_estimators': [4, 6, 9], \n",
    "              'max_features': ['log2', 'sqrt','auto'], \n",
    "              'criterion': ['entropy', 'gini'],\n",
    "              'max_depth': [2, 3, 5, 10], \n",
    "              'min_samples_split': [2, 3, 5],\n",
    "              'min_samples_leaf': [1,5,8],\n",
    "              'oob_score': [True],\n",
    "              'bootstrap': [True],\n",
    "              'random_state' : [2]\n",
    "              \n",
    "             }\n",
    "etc = run_ml(etc, params_etc, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.1, loss='deviance', max_depth=10,\n",
      "              max_features='auto', max_leaf_nodes=None,\n",
      "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "              min_samples_leaf=8, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=6, presort='auto',\n",
      "              random_state=2, subsample=0.8, verbose=0, warm_start=False)\n",
      "0.863075196409\n",
      "[  7.41199060e-02   1.03809707e-01   6.80845474e-03   9.54475875e-02\n",
      "   1.50647143e-01   2.42701092e-03   7.27508193e-02   7.05076685e-06\n",
      "   1.43198233e-03   2.14749865e-03   0.00000000e+00   8.96832430e-03\n",
      "   0.00000000e+00   2.79085350e-01   3.36667004e-02   1.70105050e-03\n",
      "   5.56285027e-03   8.06274101e-03   6.71101191e-04   0.00000000e+00\n",
      "   2.71928644e-03   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   2.77043026e-02   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   8.15967381e-03   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   6.67450853e-03\n",
      "   8.33152753e-03   5.16284262e-03   9.39325808e-02]\n"
     ]
    }
   ],
   "source": [
    "gbc = GradientBoostingClassifier()\n",
    "params_gbc = {\n",
    "    # tree params     \n",
    "    \"min_samples_split\" : [2, 3, 5],\n",
    "    \"min_samples_leaf\" : [1, 5, 8],\n",
    "    \"max_depth\" : [2, 3, 5, 10],\n",
    "    \"max_features\" : ['auto', 'sqrt', 'log2'],\n",
    "    # model building params\n",
    "    \"learning_rate\" : [0.05, 0.75, 0.1],\n",
    "    \"n_estimators\" : [4, 6, 9],\n",
    "    \"subsample\" : [0.6, 0.8, 0.9],\n",
    "    # misc params\n",
    "    \"loss\" : [\"deviance\", \"exponential\"],\n",
    "    \"random_state\" : [2]\n",
    "}\n",
    "gbc = run_ml(gbc, params_gbc, X_train, y_train)\n",
    "\n",
    "print(gbc.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict\n",
    "pred_rfc = rfc.predict(test)\n",
    "pred_etc = etc.predict(test)\n",
    "pred_gbc = gbc.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Output\n",
    "output = test[\"PassengerId\"].to_frame(\"PassengerId\")\n",
    "\n",
    "output_rfc = output.assign(Survived=pred_rfc)\n",
    "output_rfc.to_csv('output_rfc.csv', index=False)\n",
    "\n",
    "output_etc = output.assign(Survived=pred_etc)\n",
    "output_etc.to_csv('output_etc.csv', index=False)\n",
    "\n",
    "output_gbc = output.assign(Survived=pred_gbc)\n",
    "output_gbc.to_csv('output_gbc.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Top 4 kernel - kfold\n",
    "kfold = StratifiedKFold(n_splits=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 112 candidates, totalling 1120 fits\n",
      "AdaBoostClassifier(algorithm='SAMME',\n",
      "          base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='random'),\n",
      "          learning_rate=0.0001, n_estimators=1, random_state=7)\n",
      "0.769921436588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done 1120 out of 1120 | elapsed:    5.2s finished\n"
     ]
    }
   ],
   "source": [
    "# Top 4 kernel - adaboost\n",
    "dtc = DecisionTreeClassifier()\n",
    "ada_dtc = AdaBoostClassifier(dtc, random_state=7)\n",
    "ada_params = {\n",
    "    \"base_estimator__criterion\" : [\"gini\", \"entropy\"],\n",
    "    \"base_estimator__splitter\" : [\"best\", \"random\"],\n",
    "    \"algorithm\" : [\"SAMME\",\"SAMME.R\"],\n",
    "    \"n_estimators\" : [1, 2],\n",
    "    \"learning_rate\":  [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3,1.5]\n",
    "}\n",
    "gs_ada_dtc = GridSearchCV(ada_dtc, param_grid=ada_params, cv=kfold, scoring=\"accuracy\", n_jobs=2, verbose=1)\n",
    "gs_ada_dtc.fit(X_train, y_train)\n",
    "ada_best = gs_ada_dtc.best_estimator_\n",
    "print(ada_best)\n",
    "print(gs_ada_dtc.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 54 candidates, totalling 540 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    9.4s\n",
      "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:   33.1s\n",
      "[Parallel(n_jobs=2)]: Done 446 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=2)]: Done 540 out of 540 | elapsed:  1.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "           max_depth=None, max_features=3, max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=3, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=300, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "0.830527497194\n"
     ]
    }
   ],
   "source": [
    "# Top 4 kernel - extra trees\n",
    "etc = ExtraTreesClassifier()\n",
    "etc_params = {\n",
    "    \"max_depth\": [None],\n",
    "    \"max_features\": [1, 3, 10],\n",
    "    \"min_samples_split\": [2, 3, 10],\n",
    "    \"min_samples_leaf\": [1, 3, 10],\n",
    "    \"bootstrap\": [False],\n",
    "    \"n_estimators\" :[100,300],\n",
    "    \"criterion\": [\"gini\"]\n",
    "}\n",
    "gs_etc = GridSearchCV(etc, param_grid=etc_params, cv=kfold, scoring=\"accuracy\", n_jobs=2, verbose = 1)\n",
    "gs_etc.fit(X_train, y_train)\n",
    "etc_best = gs_etc.best_estimator_\n",
    "print(etc_best)\n",
    "print(gs_etc.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 54 candidates, totalling 540 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:   11.6s\n",
      "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:   34.1s\n",
      "[Parallel(n_jobs=2)]: Done 446 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=2)]: Done 540 out of 540 | elapsed:  1.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features=3, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=3, min_samples_split=3,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=300, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "0.837261503928\n"
     ]
    }
   ],
   "source": [
    "# Top 4 kernel - Random trees\n",
    "rfc = RandomForestClassifier()\n",
    "rfc_params = {\n",
    "    \"max_depth\": [None],\n",
    "    \"max_features\": [1, 3, 10],\n",
    "    \"min_samples_split\": [2, 3, 10],\n",
    "    \"min_samples_leaf\": [1, 3, 10],\n",
    "    \"bootstrap\": [False],\n",
    "    \"n_estimators\" :[100,300],\n",
    "    \"criterion\": [\"gini\"]\n",
    "}\n",
    "gs_rfc = GridSearchCV(rfc, param_grid=rfc_params, cv=kfold, scoring=\"accuracy\", n_jobs=2, verbose=1)\n",
    "gs_rfc.fit(X_train, y_train)\n",
    "rfc_best = gs_rfc.best_estimator_\n",
    "print(rfc_best)\n",
    "print(gs_rfc.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 72 candidates, totalling 720 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done  88 tasks      | elapsed:    5.7s\n",
      "[Parallel(n_jobs=2)]: Done 388 tasks      | elapsed:   24.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.1, loss='deviance', max_depth=4,\n",
      "              max_features=0.1, max_leaf_nodes=None,\n",
      "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "              min_samples_leaf=100, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=200,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "0.818181818182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done 720 out of 720 | elapsed:   45.0s finished\n"
     ]
    }
   ],
   "source": [
    "# Top 4 kernel - gradient boosting\n",
    "gbc = GradientBoostingClassifier()\n",
    "gbc_params = {\n",
    "    'loss' : [\"deviance\"],\n",
    "    'n_estimators' : [100,200,300],\n",
    "    'learning_rate': [0.1, 0.05, 0.01],\n",
    "    'max_depth': [4, 8],\n",
    "    'min_samples_leaf': [100,150],\n",
    "    'max_features': [0.3, 0.1] \n",
    "}\n",
    "gs_gbc = GridSearchCV(gbc, param_grid=gbc_params, cv=kfold, scoring=\"accuracy\", n_jobs=2, verbose=1)\n",
    "gs_gbc.fit(X_train, y_train)\n",
    "gbc_best = gs_gbc.best_estimator_\n",
    "print(gbc_best)\n",
    "print(gs_gbc.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 28 candidates, totalling 280 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    8.2s\n",
      "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:   40.8s\n",
      "[Parallel(n_jobs=2)]: Done 280 out of 280 | elapsed:  1.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=1, kernel='rbf',\n",
      "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "0.616161616162\n"
     ]
    }
   ],
   "source": [
    "# Top 4 kernel - scv classifier\n",
    "svc = SVC(probability=True)\n",
    "svc_params = {\n",
    "    'kernel': ['rbf'], \n",
    "    'gamma': [ 0.001, 0.01, 0.1, 1],\n",
    "    'C': [1, 10, 50, 100,200,300, 1000]\n",
    "}\n",
    "gs_svc = GridSearchCV(svc, param_grid=svc_params, cv=kfold, scoring=\"accuracy\", n_jobs= 2, verbose = 1)\n",
    "gs_svc.fit(X_train, y_train)\n",
    "print(svc_best)\n",
    "print(gs_svc.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Top 4 kernel - voting classifier\n",
    "vc = VotingClassifier(estimators=[(\"rfc\", rfc_best), (\"etc\", etc_best), (\"svc\", svc_best), (\"ada\", ada_best), (\"gbc\", gbc_best)], voting=\"soft\", n_jobs=2)\n",
    "vc = vc.fit(X_train, y_train)\n",
    "\n",
    "pred_vc = vc.predict(test)\n",
    "\n",
    "output = test[\"PassengerId\"].to_frame(\"PassengerId\")\n",
    "output_vc = output.assign(Survived=pred_vc)\n",
    "output_vc.to_csv('output_vc.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
